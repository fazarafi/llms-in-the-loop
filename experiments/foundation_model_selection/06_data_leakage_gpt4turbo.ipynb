{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "868423bcb2374ae9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setup & Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecb90216da6c2af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d828ce59705875e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:46.067763Z",
     "start_time": "2024-03-12T13:03:46.061093Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from contextlib import contextmanager\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from openai import APIStatusError, RateLimitError, APIConnectionError\n",
    "import numpy as np\n",
    "import time\n",
    "from getpass import getpass\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import json\n",
    "import requests\n",
    "import regex as re\n",
    "from datasets.utils import disable_progress_bar\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1685e55b343405e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:47.324506Z",
     "start_time": "2024-03-12T13:03:47.320885Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gpt-4-0125-preview -> GPT-4-Turbo\n",
    "MODEL = 'gpt-4-0125-preview'\n",
    "# Testing leakage with 0 temperature for more deterministic results\n",
    "TEMPERATURE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3dec6a47899bc239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:47.387270Z",
     "start_time": "2024-03-12T13:03:47.382984Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specifying path to the necessary files and folders\n",
    "PATH_TO_SRC = os.path.abspath('../../../')\n",
    "CONFIG_PATH = os.path.join(PATH_TO_SRC, \"settings/config.yml\")\n",
    "RESULTS_PATH = os.path.join(PATH_TO_SRC, 'data/data_leakage', MODEL)\n",
    "\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.mkdir(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "55ae1c8d9f185a42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:47.482672Z",
     "start_time": "2024-03-12T13:03:47.477636Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def extend_sys_path(path):\n",
    "    if path not in sys.path:\n",
    "        # Append the path to sys.path\n",
    "        sys.path.append(path)\n",
    "    try:\n",
    "        # Execute code inside the 'with' statement\n",
    "        yield\n",
    "    finally:\n",
    "        # Remove the path from sys.path\n",
    "        if path in sys.path:\n",
    "            sys.path.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5b675c14ed9f7362",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:47.580326Z",
     "start_time": "2024-03-12T13:03:47.577014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Temporarily add module_path and import functions\n",
    "with extend_sys_path(PATH_TO_SRC):\n",
    "    from src.utils.utils import get_api_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4a86e3e85435e6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.106571Z",
     "start_time": "2024-03-12T13:03:47.688872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init openai client\n",
    "openai_client = OpenAI(api_key=getpass(\"OPENAI API key:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4deb8673b460a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.111364Z",
     "start_time": "2024-03-12T13:03:54.108224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ASK_GPT_PARAMS = {\n",
    "    'openai_client': openai_client,\n",
    "    'model': MODEL,\n",
    "    'temperature': TEMPERATURE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1e6771b899197458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.223003Z",
     "start_time": "2024-03-12T13:03:54.209382Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading config file\n",
    "config = yaml.safe_load(open(os.path.join(PATH_TO_SRC, \"settings/config.yml\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8669e779ec7f9e3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "39768954e217f14d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.280009Z",
     "start_time": "2024-03-12T13:03:54.274010Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_from_dataset(dataset_dict, num_samples=10, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly samples a specified number of examples from each split \n",
    "    in a DatasetDict in a reproducible manner.\n",
    "    \"\"\"\n",
    "    # Set the numpy random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    sampled_dataset_dict = DatasetDict()\n",
    "\n",
    "    # Iterate over each split in the original DatasetDict\n",
    "    for split, dataset in dataset_dict.items():\n",
    "        # Generate a list of random indices with no replacement\n",
    "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "        # Select the samples corresponding to the generated indices\n",
    "        sampled_dataset = dataset.select(indices)\n",
    "\n",
    "        # Add the sampled dataset to the new DatasetDict\n",
    "        sampled_dataset_dict[split] = sampled_dataset\n",
    "\n",
    "    return sampled_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f10a279876c9dcba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.403707Z",
     "start_time": "2024-03-12T13:03:54.396465Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ask_gpt(\n",
    "        user_prompt, \n",
    "        openai_client,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        model='gpt-4-0125-preview',\n",
    "        system_prompt=None):\n",
    "\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "\n",
    "    # Save query params\n",
    "    query_params = {\n",
    "        'model': model,\n",
    "        'temperature': temperature,\n",
    "        'messages': [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                     {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        'max_tokens': max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Query the model\n",
    "        response = openai_client.chat.completions.create(**query_params)\n",
    "        # Extract model answer\n",
    "        answer = response.choices[0].message.content\n",
    "        return answer\n",
    "\n",
    "    except APIConnectionError as e:\n",
    "        raise Exception(f\"The server could not be reached: {e.__cause__}\")\n",
    "    except RateLimitError as e:\n",
    "        raise Exception(f\"A 429 status code was received: {e}\")\n",
    "    except APIStatusError as e:\n",
    "        raise Exception(f\"Non-200-range status code received: {e.status_code}, {e.response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "10807fcfbec02247",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.530894Z",
     "start_time": "2024-03-12T13:03:54.525384Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_guess_stats(answers, pattern):\n",
    "    \"\"\"\n",
    "    Computes the correct guess counts and percentages for each dictionary in answers based on a regex pattern.\n",
    "    \"\"\"\n",
    "    correct_guess_counts = []\n",
    "    correct_guess_percentages = []\n",
    "\n",
    "    for answer_dict in answers:\n",
    "        match_count = sum(answer_dict[key] for key in answer_dict if re.search(pattern, key))\n",
    "        num_records = sum(answer_dict.values())\n",
    "\n",
    "        correct_guess_counts.append(match_count)\n",
    "        correct_guess_percentages.append(match_count / num_records if num_records else 0)\n",
    "\n",
    "    mean_percentage = round(np.mean(correct_guess_percentages), 2)\n",
    "    std_deviation = round(np.std(correct_guess_percentages), 2)\n",
    "\n",
    "    return mean_percentage, std_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c5c35ed184778",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2b0895f8a2921afa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.558521Z",
     "start_time": "2024-03-12T13:03:54.555085Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Guided instruction for sentence continuation \n",
    "GUI_SENT = \"\"\"INSTRUCTION:\n",
    "You are provided with the FIRST PIECE of data sample tokens from the {split_name} split of the {dataset_name} dataset. Finish the SECOND PIECE of the sample tokens as EXACTLY appeared in the dataset. ONLY rely on the original form of the sample tokens in the dataset to finish the SECOND PIECE. Do not return labels, provide only tokens exactly as they appear in the dataset.\n",
    "\n",
    "FIRST PIECE:\n",
    "{sentence}\n",
    "\n",
    "SECOND PIECE:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c94ab11c21b12476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.614511Z",
     "start_time": "2024-03-12T13:03:54.610081Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GPT-4 evaluation prompt\n",
    "ICL_EVAL = \"\"\"INSTRUCTION:\n",
    "You are provided with a piece of reference text and a piece of candidate text.\n",
    "The candidate text is a generated replica of the reference text.\n",
    "Your task is to determine if the candidate text is an exact or near-exact match of the reference text.\n",
    "---\n",
    "Example 1:\n",
    "REFERENCE TEXT:\n",
    "The cat waited at the top.\n",
    "CANDIDATE TEXT:\n",
    "The cat waited at the top.\n",
    "\n",
    "ANSWER: Yes (exact match)\n",
    "---\n",
    "Example 2:\n",
    "REFERENCE TEXT:\n",
    "'نيرو 2'\n",
    "CANDIDATE TEXT:\n",
    "'نيرو '\n",
    "\n",
    "ANSWER: Yes (near-exact match)\n",
    "---\n",
    "Example 3:\n",
    "REFERENCE TEXT:\n",
    "Japan) Suzuki 1:27.501\n",
    "CANDIDATE TEXT:\n",
    "JPN) Kawasaki 1:27.808\n",
    "\n",
    "ANSWER: Yes (near-exact match)\n",
    "---\n",
    "Example 4:\n",
    "REFERENCE TEXT:\n",
    "1996-08-22\n",
    "CANDIDATE TEXT:\n",
    "1996-08-26\n",
    "\n",
    "ANSWER: Yes (near-exact match)\n",
    "---\n",
    "Example 5:\n",
    "REFERENCE TEXT:\n",
    "{reference_text}\n",
    "CANDIDATE TEXT:\n",
    "{candidate_text}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fb69d465c6630",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Asking LLM about data sample source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c482f12caaaef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A few previous experiments have shown that a foundation model (currently we focus on GPT-4-Turbo) answers correctly name of the dataset for randomly chosen samples from old datasets. At the same time, it is not able to answer the source of the records from our NER dataset masakhaner2.\n",
    "\n",
    "We evaluate how well the model knows the source for the random data samples from a few famous datasets and our focus dataset masakhaner2. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35ffb50d126e01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## GPT-4-Turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c52a33ebc59cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f8bf1d4f470c9663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:54.686038Z",
     "start_time": "2024-03-12T13:03:54.682873Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Identify the source NER dataset for this sample. Respond with the dataset name alone. {sentence}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e1bf3dc9d5f4814",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:03:56.194628Z",
     "start_time": "2024-03-12T13:03:54.790071Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll = load_dataset(\"conll2003\")\n",
    "\n",
    "conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "da0494b807d9d55e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:20.982029Z",
     "start_time": "2024-03-12T13:03:56.196237Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6635199383f14beaa733b69ee2b1f930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'train': {'11394': 'conll-2003',\n",
       "   '2164': 'conll-2003',\n",
       "   '7803': 'conll-2003',\n",
       "   '321': 'conll-2003',\n",
       "   '5407': 'conll-2003',\n",
       "   '12294': 'conll-2003',\n",
       "   '7545': 'conll-2003',\n",
       "   '11242': 'conll-2003',\n",
       "   '974': 'conll-2003',\n",
       "   '9011': 'conll-2003'},\n",
       "  'validation': {'1058': 'conll-2003',\n",
       "   '2570': 'conll-2003',\n",
       "   '2599': 'conll-2003',\n",
       "   '1246': 'conll-2003',\n",
       "   '430': 'conll-2003',\n",
       "   '140': 'conll-2003',\n",
       "   '1012': 'conll-2003',\n",
       "   '2185': 'conll-2003',\n",
       "   '3151': 'conll-2003',\n",
       "   '1528': 'conll-2003'},\n",
       "  'test': {'944': 'conll-03',\n",
       "   '1588': 'muc-4',\n",
       "   '4': 'conll-2003',\n",
       "   '744': 'conll 2003',\n",
       "   '906': 'conll-2003',\n",
       "   '2230': 'conll-2003',\n",
       "   '494': 'nfl scorigami',\n",
       "   '80': 'conll-2003',\n",
       "   '2878': 'conll-2003',\n",
       "   '1216': 'conll-2003'}},\n",
       " {'train': {'272': 'conll-2003',\n",
       "   '4300': 'conll-2003',\n",
       "   '11531': 'conll-2003',\n",
       "   '1060': 'conll-2003',\n",
       "   '588': 'conll-2003',\n",
       "   '13957': 'conll-2003',\n",
       "   '2797': 'conll-2003',\n",
       "   '6161': 'conll-2003',\n",
       "   '11661': 'conll-2003',\n",
       "   '8453': 'conll-2003'},\n",
       "  'validation': {'2385': 'conll-2003',\n",
       "   '1393': 'conll-2003',\n",
       "   '3236': 'conll-2003',\n",
       "   '2364': 'conll-2003',\n",
       "   '2291': 'conll-2003',\n",
       "   '2922': 'conll-2003',\n",
       "   '1959': 'conll-2003',\n",
       "   '1404': 'conll-2003',\n",
       "   '1957': 'conll 2003',\n",
       "   '272': 'conll-2003'},\n",
       "  'test': {'3429': 'conll-2003',\n",
       "   '2461': 'conll-2003',\n",
       "   '892': 'conll-2003',\n",
       "   '2045': 'conll-2003',\n",
       "   '763': 'conll-2003',\n",
       "   '1655': 'conll-2003',\n",
       "   '2406': 'conll-2003',\n",
       "   '2143': 'ace 2005',\n",
       "   '553': 'conll-2003',\n",
       "   '3052': 'conll-2003'}},\n",
       " {'train': {'11597': 'conll-2003',\n",
       "   '12836': 'conll-2003',\n",
       "   '6988': 'conll-2003',\n",
       "   '3608': 'conll-2003',\n",
       "   '3053': 'conll-2003',\n",
       "   '10571': 'conll-2003',\n",
       "   '3017': 'conll-2003',\n",
       "   '8005': 'aqmar',\n",
       "   '4342': 'conll-2003',\n",
       "   '4071': 'conll 2003'},\n",
       "  'validation': {'338': 'conll-2003',\n",
       "   '767': 'conll-2003',\n",
       "   '662': 'conll-2003',\n",
       "   '2751': 'conll-2003',\n",
       "   '3075': 'conll-2003',\n",
       "   '404': 'conll-2003',\n",
       "   '650': 'conll-2003',\n",
       "   '2408': 'conll-2003',\n",
       "   '2813': 'baseball databank',\n",
       "   '2535': 'conll-2003'},\n",
       "  'test': {'845': 'conll-2003',\n",
       "   '2784': 'conll-2003',\n",
       "   '535': 'conll-2003',\n",
       "   '2238': 'conll-2003',\n",
       "   '2581': 'conll-2003',\n",
       "   '44': 'conll-2003',\n",
       "   '1696': 'conll-2003',\n",
       "   '131': 'conll-2003',\n",
       "   '2301': 'conll-2003',\n",
       "   '2419': 'conll-2003'}}]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_experiment = 3\n",
    "conll_answers = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    # Dictionary to save current experiments results\n",
    "    experiment_answers = {}\n",
    "\n",
    "    # Random sampling of N records from each split in the dataset\n",
    "    conll_samples = sample_from_dataset(conll, num_samples=10, seed=experiment_i)\n",
    "    \n",
    "    # Iterate over each data split and its corresponding samples\n",
    "    for data_split, samples in conll_samples.items():\n",
    "        # Store answers for each split\n",
    "        experiment_answers[data_split] = {}\n",
    "    \n",
    "        # For each sample in the current data split -> ask source\n",
    "        for i, (id, tokens) in enumerate(zip(samples['id'], samples['tokens'])):\n",
    "            # Format prompt with current data sample\n",
    "            query = user_prompt.format(sentence=str(tokens))\n",
    "            experiment_answers[data_split][id] = ask_gpt(query, **ASK_GPT_PARAMS).lower()\n",
    "\n",
    "    conll_answers.append(experiment_answers)\n",
    "\n",
    "conll_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bd8597768e28f4d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:20.988778Z",
     "start_time": "2024-03-12T13:05:20.983870Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'CoNLL-2003_sample_source.json'), 'w') as file:\n",
    "    json.dump(conll_answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9aafb27398bcc350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:20.994772Z",
     "start_time": "2024-03-12T13:05:20.991366Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate number of times the data sample source was guessed correctly\n",
    "conll_answers = json.load(open(os.path.join(RESULTS_PATH, f'CoNLL-2003_sample_source.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f4b41225a4510bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.015201Z",
     "start_time": "2024-03-12T13:05:20.996468Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41429850e5d4392be0e41990752be99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'conll-2003': 26,\n",
       "  'conll-03': 1,\n",
       "  'muc-4': 1,\n",
       "  'conll 2003': 1,\n",
       "  'nfl scorigami': 1},\n",
       " {'conll-2003': 28, 'conll 2003': 1, 'ace 2005': 1},\n",
       " {'conll-2003': 27, 'aqmar': 1, 'conll 2003': 1, 'baseball databank': 1}]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times each dataset name is predicted\n",
    "conll_counts = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    conll_answers_list = []\n",
    "\n",
    "    experiment_answers = conll_answers[experiment_i]\n",
    "    for split, samples in experiment_answers.items():\n",
    "        conll_answers_list += list(samples.values())\n",
    "\n",
    "    # Append current experiment answers\n",
    "    conll_counts.append(dict(Counter(conll_answers_list)))\n",
    "\n",
    "conll_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f9d816797509c789",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.94\n",
      "Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_guess_stats(answers=conll_counts, pattern=re.compile(r'conll.*03'))\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0165f345d15d26e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### WikiAnn (multilingual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287e664d323ed8b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "WikiAnn is a dataset for cross-lingual name tagging and linking based on Wikipedia articles in 295 languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "af9670c45064ab62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.530049Z",
     "start_time": "2024-03-12T13:05:21.022509Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get languages from the dataset\n",
    "url = \"https://datasets-server.huggingface.co/splits?dataset=wikiann\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "610288777e184477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.536460Z",
     "start_time": "2024-03-12T13:05:21.531883Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ace', 'af', 'als', 'am', 'an']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiann_languages = sorted(list({item['config'] for item in data['splits']}))\n",
    "wikiann_languages[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "71bd7e55d4b8deee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.542964Z",
     "start_time": "2024-03-12T13:05:21.537644Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bh', 'et', 'sk'], dtype='<U12')"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of random languages with no replacement\n",
    "np.random.seed(42)\n",
    "num_languages = 3\n",
    "\n",
    "sampled_languages = np.random.choice(wikiann_languages, size=num_languages, replace=False)\n",
    "sampled_languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e56b9391ee8a4a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bh: Bihari languages. It's a group of languages spoken in the Bihar region of India, but \"bh\" is often used specifically to refer to Bhojpuri.\n",
    "et: Estonian. A Uralic language spoken primarily in Estonia.\n",
    "sk: Slovak. A West Slavic language spoken in Slovakia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "be94750916a60c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.550025Z",
     "start_time": "2024-03-12T13:05:21.546830Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_id(example, idx):\n",
    "    example['id'] = idx\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "98f0afd9f2897b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:05:21.554570Z",
     "start_time": "2024-03-12T13:05:21.551533Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Identify the source multilingual NER dataset for this sample. Respond with the dataset name alone. {sentence}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "958fa50139bee4d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:07:54.403794Z",
     "start_time": "2024-03-12T13:05:21.556056Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2912db6474a841d0af158732ccb2fddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/ace (download: 33.75 KiB, generated: 98.64 KiB, post-processed: Unknown size, total: 132.39 KiB) to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.55 MiB, generated: 8.02 MiB, post-processed: Unknown size, total: 10.58 MiB) to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.81 MiB, generated: 8.98 MiB, post-processed: Unknown size, total: 11.79 MiB) to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 33.75 KiB, generated: 98.64 KiB, post-processed: Unknown size, total: 132.39 KiB) to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.55 MiB, generated: 8.02 MiB, post-processed: Unknown size, total: 10.58 MiB) to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.81 MiB, generated: 8.98 MiB, post-processed: Unknown size, total: 11.79 MiB) to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 33.75 KiB, generated: 98.64 KiB, post-processed: Unknown size, total: 132.39 KiB) to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c65ce95d64b276e7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.55 MiB, generated: 8.02 MiB, post-processed: Unknown size, total: 10.58 MiB) to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-abae0dbda02627e3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset None/ace (download: 2.81 MiB, generated: 8.98 MiB, post-processed: Unknown size, total: 11.79 MiB) to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-017d20d6290acf07/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'bh': {208: 'wikiann',\n",
       "   188: 'wikiann',\n",
       "   12: 'wikiann',\n",
       "   221: 'wikiann',\n",
       "   239: 'wikiann',\n",
       "   136: 'wikiann',\n",
       "   230: 'wikiann',\n",
       "   206: 'wikiann',\n",
       "   52: 'wikiann',\n",
       "   108: 'wikiann'},\n",
       "  'et': {32940: 'conll-2002 and conll-2003',\n",
       "   9739: 'wikiann',\n",
       "   23365: 'wikiann',\n",
       "   31332: 'wikiann (pan-x)',\n",
       "   34227: 'wikiann',\n",
       "   441: 'wikiann',\n",
       "   28942: 'wikiann',\n",
       "   788: 'wikiann',\n",
       "   29813: 'wikiann',\n",
       "   10314: 'wikiann'},\n",
       "  'sk': {12836: 'wikiann',\n",
       "   10913: 'multilingual bert (mbert) dataset',\n",
       "   4214: 'wikiann',\n",
       "   8198: 'wikiann (pan-x)',\n",
       "   31403: 'wikiann',\n",
       "   13917: 'wikiann',\n",
       "   27440: 'conll-2003',\n",
       "   11667: 'wikiann',\n",
       "   29616: 'wikiann',\n",
       "   39864: 'wikiann'}},\n",
       " {'bh': {189: 'wikiann',\n",
       "   123: 'wikiann',\n",
       "   185: 'wikiann',\n",
       "   213: 'wikiann',\n",
       "   106: 'wikiann',\n",
       "   127: 'wikiann',\n",
       "   176: 'wikiann',\n",
       "   73: 'wikiann',\n",
       "   275: 'wikiann',\n",
       "   242: 'wikiann'},\n",
       "  'et': {25474: 'wikiann',\n",
       "   19761: 'masader',\n",
       "   32740: 'wikiann (pan-x)',\n",
       "   34148: 'wikiann',\n",
       "   5218: 'wikiann',\n",
       "   17371: 'wikiann',\n",
       "   11160: 'wikiann (pan-x)',\n",
       "   2445: 'wikiann',\n",
       "   27584: 'wikiann',\n",
       "   4545: 'wikiann'},\n",
       "  'sk': {3841: 'wikiann (pan-x)',\n",
       "   12898: 'masakhaner',\n",
       "   15032: 'wikiann',\n",
       "   36781: 'wikiann',\n",
       "   9201: 'wikiann',\n",
       "   21288: 'wikiann',\n",
       "   37321: 'xtreme',\n",
       "   8600: 'xtreme',\n",
       "   33089: 'wikiann',\n",
       "   39511: 'conll-2002'}},\n",
       " {'bh': {98: 'wikiann',\n",
       "   259: 'wikiann',\n",
       "   184: 'wikiann',\n",
       "   256: 'wikiann',\n",
       "   29: 'wikiann',\n",
       "   254: 'wikiann',\n",
       "   7: 'wikiann',\n",
       "   13: 'wikiann',\n",
       "   230: 'wikiann (pan-x)',\n",
       "   91: 'wikiann'},\n",
       "  'et': {32557: 'conll-2002',\n",
       "   14168: 'wikiann (pan-x)',\n",
       "   21315: 'conll-2002',\n",
       "   34768: 'conll-2002',\n",
       "   33490: 'wikiann',\n",
       "   34404: 'conll-2002',\n",
       "   23350: 'conll-2002',\n",
       "   2999: 'conll-2002',\n",
       "   3719: 'wikiann (pan-x)',\n",
       "   11340: 'conll-2002'},\n",
       "  'sk': {2727: 'wikiann',\n",
       "   26907: 'wikiann',\n",
       "   16283: 'wikiann',\n",
       "   31845: 'conll-2003',\n",
       "   19862: 'wikiann (pan-x)',\n",
       "   38929: 'wikiann',\n",
       "   8545: 'conll-2002',\n",
       "   19447: 'conll-2002',\n",
       "   27187: 'wikiann',\n",
       "   5471: 'masakhaner'}}]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_experiment = 3\n",
    "wikiann_answers = []\n",
    "\n",
    "# Disable datasets load_dataset progress bar\n",
    "disable_progress_bar()\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    experiment_answers = {}\n",
    "\n",
    "    for i, lan in enumerate(sampled_languages):\n",
    "        # Load wikiann split for each of the sampled languages\n",
    "        wikiann = load_dataset(\"wikiann\", data_dir=lan, \n",
    "                               download_mode=\"force_redownload\",\n",
    "                               verification_mode=\"no_checks\")\n",
    "    \n",
    "        # Concatenate train, test, and validation splits \n",
    "        wikiann = DatasetDict({\n",
    "            \"merged\": concatenate_datasets([wikiann['train'], wikiann['test'], wikiann['validation']])\n",
    "        })\n",
    "        # Add ids per records based on the index of the record\n",
    "        wikiann = wikiann.map(add_id, with_indices=True)\n",
    "        # Sample\n",
    "        wikiann_samples = sample_from_dataset(wikiann, 10, seed=experiment_i)['merged']\n",
    "\n",
    "        experiment_answers[lan] = {}\n",
    "        # Ask source for the sampled records\n",
    "        for id_, tokens in zip(wikiann_samples['id'], wikiann_samples['tokens']):\n",
    "            # Format prompt with current data sample\n",
    "            query = user_prompt.format(sentence=str(tokens))\n",
    "            experiment_answers[lan][id_] = ask_gpt(query, **ASK_GPT_PARAMS).lower()\n",
    "\n",
    "    wikiann_answers.append(experiment_answers)\n",
    "\n",
    "wikiann_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4f1aae6ea64248a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:07:54.408379Z",
     "start_time": "2024-03-12T13:07:54.405051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'WikiAnn_sample_source.json'), 'w') as file:\n",
    "    json.dump(wikiann_answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7f3180b5aa887d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:07:54.426149Z",
     "start_time": "2024-03-12T13:07:54.409459Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24d2817895840689575f20c36333435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'wikiann': 25,\n",
       "  'conll-2002 and conll-2003': 1,\n",
       "  'wikiann (pan-x)': 2,\n",
       "  'multilingual bert (mbert) dataset': 1,\n",
       "  'conll-2003': 1},\n",
       " {'wikiann': 22,\n",
       "  'masader': 1,\n",
       "  'wikiann (pan-x)': 3,\n",
       "  'masakhaner': 1,\n",
       "  'xtreme': 2,\n",
       "  'conll-2002': 1},\n",
       " {'wikiann': 15,\n",
       "  'wikiann (pan-x)': 4,\n",
       "  'conll-2002': 9,\n",
       "  'conll-2003': 1,\n",
       "  'masakhaner': 1}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate number of times the data sample source was guessed correctly\n",
    "wikiann_answers = json.load(open(os.path.join(RESULTS_PATH, f'WikiAnn_sample_source.json'), 'r'))\n",
    "\n",
    "# Count number of times each dataset name is predicted\n",
    "wikiann_counts = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    wikiann_answers_list = []\n",
    "\n",
    "    experiment_answers = wikiann_answers[experiment_i]\n",
    "    for split, samples in experiment_answers.items():\n",
    "        wikiann_answers_list += list(samples.values())\n",
    "\n",
    "    # Append current experiment answers\n",
    "    wikiann_counts.append(dict(Counter(wikiann_answers_list)))\n",
    "\n",
    "wikiann_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1db03e1aa9a3cdc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.81\n",
      "Std: 0.13\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_guess_stats(answers=wikiann_counts, pattern=re.compile(r'(wikiann.*|xtreme)'))\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c94b8f4f583e3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Wikiann (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3e95aadfd99c5788",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Identify the source multilingual NER dataset for this sample. Respond with the dataset name alone. {sentence}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "998c8497250237a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:08:02.236568Z",
     "start_time": "2024-03-12T13:07:54.436125Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/ace (download: 2.85 MiB, generated: 8.92 MiB, post-processed: Unknown size, total: 11.78 MiB) to /root/.cache/huggingface/datasets/parquet/ace-ea28d0999d5d2e61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-ea28d0999d5d2e61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "wikiann = load_dataset(\"wikiann\", data_dir='en',\n",
    "                       download_mode=\"force_redownload\",\n",
    "                       verification_mode=\"no_checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "da8f88f4b2bea5c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.607961Z",
     "start_time": "2024-03-12T13:08:02.238839Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5abc72d02f45aaba35e2978a5b80ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'train': {19134: 'wikiann',\n",
       "   4981: 'wikiann',\n",
       "   16643: 'wikiann',\n",
       "   19117: 'wikiann',\n",
       "   5306: 'pan-x',\n",
       "   230: 'wikiann',\n",
       "   3148: 'pan-x',\n",
       "   11525: 'wikiann',\n",
       "   13672: 'conll-2002',\n",
       "   1624: 'wikiann'},\n",
       "  'validation': {8165: 'wikiann',\n",
       "   2991: 'conll-2002 and conll-2003',\n",
       "   3790: 'xtreme',\n",
       "   151: 'wikiann',\n",
       "   18: 'wikiann',\n",
       "   3625: 'masakhaner 2.0',\n",
       "   2436: 'wikiann',\n",
       "   8198: 'conll-2002 and conll-2003',\n",
       "   2861: 'wikiann (pan-x)',\n",
       "   3539: 'conll-2002 and conll-2003'},\n",
       "  'test': {6503: 'masakhaner',\n",
       "   4455: 'wikiann',\n",
       "   9053: 'conll-2002 and conll-2003',\n",
       "   858: 'wikiann',\n",
       "   1333: 'wikiann',\n",
       "   2354: 'wikiann',\n",
       "   1470: 'wikiann',\n",
       "   2913: 'conll-2002',\n",
       "   4010: 'wikiann',\n",
       "   4974: 'wikiann'}},\n",
       " {'train': {11456: 'wikiann (pan-x)',\n",
       "   16528: 'conll-2003',\n",
       "   3253: 'wikiann',\n",
       "   18614: 'conll-2002',\n",
       "   1544: 'wikiann',\n",
       "   12568: 'conll-2002',\n",
       "   15497: 'conll-2002 and conll-2003',\n",
       "   13987: 'wikiann',\n",
       "   9598: 'conll-2002 and conll-2003',\n",
       "   6668: 'wikiann'},\n",
       "  'validation': {8094: 'wikiann',\n",
       "   3667: 'wikiann',\n",
       "   7979: 'conll-2002',\n",
       "   8585: 'wikiann',\n",
       "   2940: 'masakhaner',\n",
       "   8457: 'conll-2002',\n",
       "   4399: 'conll-2002 and conll-2003',\n",
       "   3681: 'wikiann',\n",
       "   2150: 'wikiann',\n",
       "   910: 'conll-2002'},\n",
       "  'test': {5025: 'wikiann',\n",
       "   2998: 'conll-2002',\n",
       "   1844: 'wikiann',\n",
       "   2451: 'wikiann',\n",
       "   4828: 'wikiann',\n",
       "   169: 'conll-2002 and conll-2003',\n",
       "   6316: 'conll-2002',\n",
       "   4380: 'wikiann',\n",
       "   850: 'wikiann (pan-x)',\n",
       "   1646: 'wikiann'}},\n",
       " {'train': {836: 'wikiann',\n",
       "   1849: 'wikiann',\n",
       "   16673: 'wikiann',\n",
       "   527: 'wikiann',\n",
       "   19846: 'wikiann',\n",
       "   1497: 'wikiann',\n",
       "   16622: 'xtreme',\n",
       "   5043: 'wikiann',\n",
       "   16981: 'wikiann',\n",
       "   19634: 'wikiann'},\n",
       "  'validation': {510: 'wikiann',\n",
       "   3721: 'wikiann',\n",
       "   8276: 'wikiann (pan-x)',\n",
       "   441: 'masakhaner',\n",
       "   4430: 'conll-2002',\n",
       "   3853: 'conll-2003',\n",
       "   3713: 'wikiann',\n",
       "   9109: 'wikiann',\n",
       "   1893: 'conll-2002 and conll-2003',\n",
       "   3810: 'wikiann'},\n",
       "  'test': {54: 'wikiann',\n",
       "   9142: 'wikiann',\n",
       "   4472: 'masakhaner 2.0',\n",
       "   3535: 'wikiann (pan-x)',\n",
       "   2042: 'conll-2002 and conll-2003',\n",
       "   6145: 'xtreme',\n",
       "   729: 'wikiann',\n",
       "   2317: 'wikiann',\n",
       "   5972: 'wikiann',\n",
       "   1310: 'wikiann'}}]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_experiment = 3\n",
    "wikiann_answers = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    # Dictionary to save current experiments results\n",
    "    experiment_answers = {}\n",
    "\n",
    "    wikiann = wikiann.map(add_id, with_indices=True)\n",
    "\n",
    "    # Random sampling of N records from each split in the dataset\n",
    "    wikiann_samples = sample_from_dataset(wikiann, num_samples=10, seed=experiment_i)\n",
    "\n",
    "    # Iterate over each data split and its corresponding samples\n",
    "    for data_split, samples in wikiann_samples.items():\n",
    "        # Store answers for each split\n",
    "        experiment_answers[data_split] = {}\n",
    "\n",
    "        # For each sample in the current data split -> ask source\n",
    "        for i, (id, tokens) in enumerate(zip(samples['id'], samples['tokens'])):\n",
    "            # Format prompt with current data sample\n",
    "            query = user_prompt.format(sentence=str(tokens))\n",
    "            experiment_answers[data_split][id] = ask_gpt(query, **ASK_GPT_PARAMS).lower()\n",
    "\n",
    "    wikiann_answers.append(experiment_answers)\n",
    "\n",
    "wikiann_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2e48241e43881a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.614540Z",
     "start_time": "2024-03-12T13:09:20.609775Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'WikiAnn_en_sample_source.json'), 'w') as file:\n",
    "    json.dump(wikiann_answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c0cf5bd45d13fa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.637849Z",
     "start_time": "2024-03-12T13:09:20.615848Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f644f28e72e045d9b9810261cffdb69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'wikiann': 18,\n",
       "  'pan-x': 2,\n",
       "  'conll-2002': 2,\n",
       "  'conll-2002 and conll-2003': 4,\n",
       "  'xtreme': 1,\n",
       "  'masakhaner 2.0': 1,\n",
       "  'wikiann (pan-x)': 1,\n",
       "  'masakhaner': 1},\n",
       " {'wikiann (pan-x)': 2,\n",
       "  'conll-2003': 1,\n",
       "  'wikiann': 15,\n",
       "  'conll-2002': 7,\n",
       "  'conll-2002 and conll-2003': 4,\n",
       "  'masakhaner': 1},\n",
       " {'wikiann': 20,\n",
       "  'xtreme': 2,\n",
       "  'wikiann (pan-x)': 2,\n",
       "  'masakhaner': 1,\n",
       "  'conll-2002': 1,\n",
       "  'conll-2003': 1,\n",
       "  'conll-2002 and conll-2003': 2,\n",
       "  'masakhaner 2.0': 1}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate number of times the data sample source was guessed correctly\n",
    "wikiann_answers = json.load(open(os.path.join(RESULTS_PATH, f'WikiAnn_en_sample_source.json'), 'r'))\n",
    "\n",
    "# Count number of times each dataset name is predicted\n",
    "wikiann_counts = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    wikiann_answers_list = []\n",
    "\n",
    "    experiment_answers = wikiann_answers[experiment_i]\n",
    "    for split, samples in experiment_answers.items():\n",
    "        wikiann_answers_list += list(samples.values())\n",
    "\n",
    "    # Append current experiment answers\n",
    "    wikiann_counts.append(dict(Counter(wikiann_answers_list)))\n",
    "\n",
    "wikiann_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "cd1f4b8b4a6f3623",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.644346Z",
     "start_time": "2024-03-12T13:09:20.639152Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7\n",
      "Std: 0.1\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_guess_stats(answers=wikiann_counts, pattern=re.compile(r'(wikiann.*|xtreme|pan-x)'))\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a633faef4411ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### masakhaner2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f56fd31f81e2cc00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.651381Z",
     "start_time": "2024-03-12T13:09:20.645815Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['xho', 'ewe', 'nya'], dtype='<U3')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Languages that were added to the second version of the masakhaner2\n",
    "masakhaner2_languages = ['bam', 'ewe', 'fon', 'twi', 'bbj', 'nya', 'tsn', 'sna', 'xho', 'zul']\n",
    "\n",
    "# Generate a list of random languages \n",
    "np.random.seed(42)\n",
    "num_languages = 3\n",
    "\n",
    "sampled_languages = np.random.choice(masakhaner2_languages, size=num_languages, replace=False)\n",
    "sampled_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a82663110a82a74e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:09:20.655297Z",
     "start_time": "2024-03-12T13:09:20.652528Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Identify the source multilingual NER dataset for this sample. Respond with the dataset name alone. {sentence}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c8db58e29f93a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:10:58.086357Z",
     "start_time": "2024-03-12T13:09:20.656637Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32b42255778400bafb897fe7a4f5e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/xho/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/nya/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/xho/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/nya/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/xho/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n",
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/nya/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'xho': {'3341': 'masakhanener',\n",
       "   '62': 'masakhanener',\n",
       "   '811': 'masakhanener',\n",
       "   '2602': 'masakhaner',\n",
       "   '4397': 'masakhanener',\n",
       "   '1123': 'masakhanener',\n",
       "   '449': 'masakhaner',\n",
       "   '341': 'masakhaner 2.0',\n",
       "   '108': 'masakhaner 2.0',\n",
       "   '2534': 'masakhanener'},\n",
       "  'ewe': {'472': 'masakhaner',\n",
       "   '2086': 'masakhaner',\n",
       "   '985': 'masakhaner',\n",
       "   '125': 'masakhanener',\n",
       "   '2995': 'masakhaner',\n",
       "   '450': 'masakhaner',\n",
       "   '584': 'masakhaner',\n",
       "   '2317': 'masakhaner',\n",
       "   '422': 'masakhaner',\n",
       "   '2077': 'masakhaner'},\n",
       "  'nya': {'222': 'masakhanener',\n",
       "   '538': 'masakhaner',\n",
       "   '575': 'masakhaner',\n",
       "   '1603': 'masakhaner ',\n",
       "   '2603': 'masakhaner',\n",
       "   '199': 'masakhaner',\n",
       "   '1356': 'masakhaner',\n",
       "   '267': 'masakhaner',\n",
       "   '1216': 'masakhaner',\n",
       "   '3045': 'masakhanener'}},\n",
       " {'xho': {'3341': 'masakhanener',\n",
       "   '62': 'masakhanener',\n",
       "   '811': 'masakhaner 2.0',\n",
       "   '2602': 'masakhaner',\n",
       "   '4397': 'masakhanener',\n",
       "   '1123': 'masakhanener',\n",
       "   '449': 'masakhaner',\n",
       "   '341': 'masakhanener',\n",
       "   '108': 'masakhanener',\n",
       "   '2534': 'masakhaner'},\n",
       "  'ewe': {'472': 'masakhaner',\n",
       "   '2086': 'masakhaner',\n",
       "   '985': 'masakhaner',\n",
       "   '125': 'masakhanener',\n",
       "   '2995': 'masakhanener',\n",
       "   '450': 'masakhaner',\n",
       "   '584': 'masakhaner',\n",
       "   '2317': 'masakhaner',\n",
       "   '422': 'masakhaner',\n",
       "   '2077': 'masakhaner'},\n",
       "  'nya': {'222': 'masakhanener',\n",
       "   '538': 'masakhaner',\n",
       "   '575': 'masakhaner',\n",
       "   '1603': 'masakhanener',\n",
       "   '2603': 'masakhaner',\n",
       "   '199': 'masakhaner',\n",
       "   '1356': 'masakhaner',\n",
       "   '267': 'masakhaner',\n",
       "   '1216': 'masakhanener',\n",
       "   '3045': 'masakhaner'}},\n",
       " {'xho': {'3341': 'masakhanener',\n",
       "   '62': 'masakhanener',\n",
       "   '811': 'masakhaner 2.0',\n",
       "   '2602': 'masakhanener',\n",
       "   '4397': 'masakhanener',\n",
       "   '1123': 'masakhanener',\n",
       "   '449': 'masakhaner',\n",
       "   '341': 'masakhanener',\n",
       "   '108': 'masakhanener',\n",
       "   '2534': 'masakhanener'},\n",
       "  'ewe': {'472': 'masakhaner',\n",
       "   '2086': 'masakhaner',\n",
       "   '985': 'masakhaner',\n",
       "   '125': 'masakhaner',\n",
       "   '2995': 'masakhanener',\n",
       "   '450': 'masakhaner',\n",
       "   '584': 'masakhaner',\n",
       "   '2317': 'masakhaner',\n",
       "   '422': 'masakhaner',\n",
       "   '2077': 'masakhaner'},\n",
       "  'nya': {'222': 'masakhanener',\n",
       "   '538': 'masakhaner',\n",
       "   '575': 'masakhaner',\n",
       "   '1603': 'masakhaner',\n",
       "   '2603': 'masakhaner',\n",
       "   '199': 'masakhaner',\n",
       "   '1356': 'masakhaner',\n",
       "   '267': 'masakhaner',\n",
       "   '1216': 'masakhaner',\n",
       "   '3045': 'masakhaner'}}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeat_experiment = 3\n",
    "masakhaner2_answers = []\n",
    "\n",
    "# Disable datasets load_dataset progress bar\n",
    "disable_progress_bar()\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    experiment_answers = {}\n",
    "\n",
    "    for i, lan in enumerate(sampled_languages):\n",
    "        # Load wikiann split for each of the sampled languages\n",
    "        masaknaner2 = load_dataset(config['dataset'], lan)\n",
    "\n",
    "        # Concatenate train, test, and validation splits \n",
    "        masaknaner2 = DatasetDict({\n",
    "            \"merged\": concatenate_datasets(\n",
    "                [masaknaner2['train'], masaknaner2['test'], masaknaner2['validation']])\n",
    "        })\n",
    "        # Sample\n",
    "        masaknaner2_samples = sample_from_dataset(masaknaner2, 10)['merged']\n",
    "\n",
    "        experiment_answers[lan] = {}\n",
    "        # Ask source for the sampled records\n",
    "        for id_, tokens in zip(masaknaner2_samples['id'], masaknaner2_samples['tokens']):\n",
    "            # Format prompt with current data sample\n",
    "            query = user_prompt.format(sentence=str(tokens))\n",
    "            experiment_answers[lan][id_] = ask_gpt(query, **ASK_GPT_PARAMS).lower()\n",
    "\n",
    "    masakhaner2_answers.append(experiment_answers)\n",
    "\n",
    "masakhaner2_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "7d4aa46f4e471a99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:10:58.092573Z",
     "start_time": "2024-03-12T13:10:58.088021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'masakhaner2_sample_source.json'), 'w') as file:\n",
    "    json.dump(masakhaner2_answers, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bcb47e913b31d100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:10:58.114822Z",
     "start_time": "2024-03-12T13:10:58.093985Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c219da3a3e18409bb3c76915bc0006c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Experiment #:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'masakhanener': 9, 'masakhaner': 18, 'masakhaner 2.0': 2, 'masakhaner ': 1},\n",
       " {'masakhanener': 11, 'masakhaner 2.0': 1, 'masakhaner': 18},\n",
       " {'masakhanener': 10, 'masakhaner 2.0': 1, 'masakhaner': 19}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate number of times the data sample source was guessed correctly\n",
    "masakhaner2_answers = json.load(open(os.path.join(RESULTS_PATH, f'masakhaner2_sample_source.json'), 'r'))\n",
    "\n",
    "# Count number of times each dataset name is predicted\n",
    "masakhaner2_counts = []\n",
    "\n",
    "for experiment_i in tqdm(range(repeat_experiment), desc=\"Experiment #\"):\n",
    "    masakhaner2_answers_list = []\n",
    "\n",
    "    experiment_answers = masakhaner2_answers[experiment_i]\n",
    "    for split, samples in experiment_answers.items():\n",
    "        masakhaner2_answers_list += list(samples.values())\n",
    "\n",
    "    # Append current experiment answers\n",
    "    masakhaner2_counts.append(dict(Counter(masakhaner2_answers_list)))\n",
    "\n",
    "masakhaner2_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "abaca3cf05c7c01c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.04\n",
      "Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "mean, std = compute_guess_stats(answers=masakhaner2_counts, pattern=re.compile(r'masakhaner 2.0'))\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2dca894790fb450a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:10:58.127952Z",
     "start_time": "2024-03-12T13:10:58.123422Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.96\n",
      "Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of times the data sample source was guessed incorrectly\n",
    "mean, std = compute_guess_stats(answers=masakhaner2_counts, \n",
    "                                pattern=re.compile(r'(masakhanener ?$|masakhaner ?$)'))\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ce3fa843b0cbba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Asking the model to continue sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e3beba71f2b7b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Following the approach suggested here: https://arxiv.org/abs/2308.08493"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688df8d18329985",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### CoNLL-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c3ddf83441c59863",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:10:59.516420Z",
     "start_time": "2024-03-12T13:10:58.129654Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll = load_dataset(\"conll2003\")\n",
    "conll_samples = sample_from_dataset(conll, num_samples=10)['train']\n",
    "conll_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2f05fc497a7877e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:12.525565Z",
     "start_time": "2024-03-12T13:10:59.517789Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd897392f897455bb55017029c6ee5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "results = {}\n",
    "\n",
    "for id_, tokens in tqdm(zip(conll_samples['id'], conll_samples['tokens']), total=10):\n",
    "    results[id_] = {}\n",
    "    \n",
    "    # Cut-off - at least 2 tokens at the beginning\n",
    "    cut_off = int(np.ceil(len(tokens)/2))\n",
    "\n",
    "    query_text = ' '.join(tokens[:cut_off])\n",
    "    reference_text = ' '.join(tokens[cut_off:])\n",
    "    \n",
    "    user_prompt = GUI_SENT.format(\n",
    "        split_name='train', \n",
    "        dataset_name='CoNLL-2003',\n",
    "        sentence=query_text\n",
    "    )\n",
    "     \n",
    "    answer = ask_gpt(user_prompt, **ASK_GPT_PARAMS)\n",
    "\n",
    "    results[id_]['query_text'] = query_text\n",
    "    results[id_]['reference_text'] = reference_text\n",
    "    results[id_]['answer'] = answer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a17efdf617e4a785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:12.532721Z",
     "start_time": "2024-03-12T13:11:12.527682Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'CoNLL-2003_sentence_continuation.json'), 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "77d4ecce8668d1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:12.538158Z",
     "start_time": "2024-03-12T13:11:12.534209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = json.load(open(os.path.join(RESULTS_PATH, f'CoNLL-2003_sentence_continuation.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ca4e27f1952205ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:12.556438Z",
     "start_time": "2024-03-12T13:11:12.539644Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27df711c447b4eba8a758d112e6de2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Start of the sentence:\n",
      " Attendance :\n",
      "--> Reference (ground truth):\n",
      "1,800\n",
      "--> Model's prediction:\n",
      "at the event was over 20,000.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Ties between the two neighbours , strained also over a military accord between Turkey and Israel which drew strong\n",
      "--> Reference (ground truth):\n",
      "Iranian objections , have improved since Islamist Necmettin Erbakan took over as Turkish prime minister in June .\n",
      "--> Model's prediction:\n",
      "criticism from Iran , have been further complicated by Ankara 's recent decision to allow Israeli aircraft to use its airspace for training , the newspaper said .\n",
      "\n",
      "--> Start of the sentence:\n",
      " The winner has to secure a majority from the college within two rounds\n",
      "--> Reference (ground truth):\n",
      "of voting otherwise the election will go back before the parliament .\n",
      "--> Model's prediction:\n",
      "of voting, according to the electoral rules.\n",
      "\n",
      "--> Start of the sentence:\n",
      " LIBERIA , Costa\n",
      "--> Reference (ground truth):\n",
      "Rica 1996-08-26\n",
      "--> Model's prediction:\n",
      "Rica 1996-08-22\n",
      "\n",
      "--> Start of the sentence:\n",
      " 1. Sorensen\n",
      "--> Reference (ground truth):\n",
      "11.20:33\n",
      "--> Model's prediction:\n",
      "has\n",
      "\n",
      "--> Start of the sentence:\n",
      " 9. Akira Ryo (\n",
      "--> Reference (ground truth):\n",
      "Japan ) Kawasaki 38:50.269\n",
      "--> Model's prediction:\n",
      "Suzuki ) 0000 Oulu , Finland 22-23 July 1998 - 10.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Six months to September\n",
      "--> Reference (ground truth):\n",
      "30 , 1996\n",
      "--> Model's prediction:\n",
      "30 1996\n",
      "\n",
      "--> Start of the sentence:\n",
      " P1020 ingot 75\n",
      "--> Reference (ground truth):\n",
      "cents / pound\n",
      "--> Model's prediction:\n",
      "tonnes to South Korea for January shipment , traders said .\n",
      "\n",
      "--> Start of the sentence:\n",
      " Sale Amount\n",
      "--> Reference (ground truth):\n",
      ": 4,300,000\n",
      "--> Model's prediction:\n",
      "of Doubles in Britain\n",
      "\n",
      "--> Start of the sentence:\n",
      " The use of riverways in the region has been made a priority under a government plan for the Amazon\n",
      "--> Reference (ground truth):\n",
      "and the high-speed hovercraft will help reduce the time involved in travelling often massive distances , it said .\n",
      "--> Model's prediction:\n",
      "Basin, environmental officials said Thursday.\n"
     ]
    }
   ],
   "source": [
    "for id_, results_dict in tqdm(results.items()):\n",
    "    print('--> Start of the sentence:\\n', results_dict['query_text'])\n",
    "    print(\"--> Reference (ground truth):\")\n",
    "    print(results_dict['reference_text'])\n",
    "    print(\"--> Model's prediction:\")\n",
    "    print(results_dict['answer'])\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8286a9561c2c36f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:18.565527Z",
     "start_time": "2024-03-12T13:11:12.558041Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf1f96c3cc4940a786f231227aa5623c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'No': 8, 'Yes (near-exact match)': 2})"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_arr = []\n",
    "\n",
    "for id_, results_dict in tqdm(results.items(), total=10):\n",
    "    \n",
    "    user_prompt = ICL_EVAL.format(\n",
    "        reference_text=results_dict['reference_text'],\n",
    "        candidate_text=results_dict['answer']\n",
    "    )\n",
    "    answer = ask_gpt(user_prompt, openai_client, temperature=TEMPERATURE, model='gpt-4')\n",
    "    results[id_]['match'] = answer\n",
    "\n",
    "    match_arr.append(answer)\n",
    "    \n",
    "Counter(match_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd9947c242927c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### WikiANN (Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b92a76c0831e6dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:11:19.152357Z",
     "start_time": "2024-03-12T13:11:18.567684Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get languages from the dataset\n",
    "url = \"https://datasets-server.huggingface.co/splits?dataset=wikiann\"\n",
    "\n",
    "# Send a GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")\n",
    "\n",
    "wikiann_languages = sorted(list({item['config'] for item in data['splits']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6c894b5fc52bcada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:12:45.907741Z",
     "start_time": "2024-03-12T13:11:19.154358Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706d2e78028340108d368e9dd8c2f87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/ace (download: 33.18 KiB, generated: 79.23 KiB, post-processed: Unknown size, total: 112.40 KiB) to /root/.cache/huggingface/datasets/parquet/ace-c175dd3637ea0c5e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c175dd3637ea0c5e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Klimop ( Ôostkamp )\n",
      "Downloading and preparing dataset None/ace (download: 26.40 KiB, generated: 66.69 KiB, post-processed: Unknown size, total: 93.09 KiB) to /root/.cache/huggingface/datasets/parquet/ace-9f12e683d0704dd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-9f12e683d0704dd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Noruwega nag-angkon sa maong dapit .\n",
      "Downloading and preparing dataset None/ace (download: 2.46 MiB, generated: 8.93 MiB, post-processed: Unknown size, total: 11.39 MiB) to /root/.cache/huggingface/datasets/parquet/ace-f6d90983d029509f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-f6d90983d029509f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "روبرت دي نيرو 2\n",
      "Downloading and preparing dataset None/ace (download: 2.10 MiB, generated: 7.29 MiB, post-processed: Unknown size, total: 9.39 MiB) to /root/.cache/huggingface/datasets/parquet/ace-38ff742c0b31abe7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-38ff742c0b31abe7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Kabupaten Manggarai ( 14-09-2015 ) ,\n",
      "Downloading and preparing dataset None/ace (download: 27.19 KiB, generated: 51.21 KiB, post-processed: Unknown size, total: 78.40 KiB) to /root/.cache/huggingface/datasets/parquet/ace-c71fce17ed8efad3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c71fce17ed8efad3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Â-lî-sân Koet-kâ Sêm-lìm Yù-lo̍k-khî\n",
      "Downloading and preparing dataset None/ace (download: 27.93 KiB, generated: 44.78 KiB, post-processed: Unknown size, total: 72.71 KiB) to /root/.cache/huggingface/datasets/parquet/ace-ef5bb871b53b8cd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-ef5bb871b53b8cd7/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "' '' 艮自侯 站 '' '（Gants 倫敦地鐵嗰\n",
      "Downloading and preparing dataset None/ace (download: 615.86 KiB, generated: 2.68 MiB, post-processed: Unknown size, total: 3.28 MiB) to /root/.cache/huggingface/datasets/parquet/ace-e7b8ceeb4277a560/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-e7b8ceeb4277a560/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Taith Bryn Euryn .\n",
      "Downloading and preparing dataset None/ace (download: 34.18 KiB, generated: 105.85 KiB, post-processed: Unknown size, total: 140.03 KiB) to /root/.cache/huggingface/datasets/parquet/ace-77356950b3ab7c3d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-77356950b3ab7c3d/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Lièng-hăk-guók : Dṳ̆ng-ùng Ngṳ̄-ngiòng Nĭk\n",
      "Downloading and preparing dataset None/ace (download: 31.20 KiB, generated: 178.44 KiB, post-processed: Unknown size, total: 209.63 KiB) to /root/.cache/huggingface/datasets/parquet/ace-92d965d1be82c0fc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-92d965d1be82c0fc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "一 ︰ 紐 約 市 ；\n",
      "Downloading and preparing dataset None/ace (download: 2.81 MiB, generated: 8.98 MiB, post-processed: Unknown size, total: 11.79 MiB) to /root/.cache/huggingface/datasets/parquet/ace-c4e351a8849ca20f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-c4e351a8849ca20f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "Do seniorského mužstva FC Nitra sa dostal ako 18 ročný .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vls': {'query_text': 'Klimop (',\n",
       "  'reference_text': 'Ôostkamp )',\n",
       "  'answer': 'Hedera)'},\n",
       " 'ceb': {'query_text': 'Noruwega nag-angkon sa',\n",
       "  'reference_text': 'maong dapit .',\n",
       "  'answer': 'iho sa Svalbard sukad pa sa 1920.'},\n",
       " 'ar': {'query_text': 'روبرت دي',\n",
       "  'reference_text': 'نيرو 2',\n",
       "  'answer': 'نيرو'},\n",
       " 'id': {'query_text': 'Kabupaten Manggarai (',\n",
       "  'reference_text': '14-09-2015 ) ,',\n",
       "  'answer': 'Nusa Tenggara Timur)'},\n",
       " 'hak': {'query_text': 'Â-lî-sân Koet-kâ',\n",
       "  'reference_text': 'Sêm-lìm Yù-lo̍k-khî',\n",
       "  'answer': 'thài-kok'},\n",
       " 'gan': {'query_text': \"' '' 艮自侯 站\",\n",
       "  'reference_text': \"'' '（Gants 倫敦地鐵嗰\",\n",
       "  'answer': '在 北京市 东城区 ， 是 一座 北京地铁 2号线 的 地铁站 。'},\n",
       " 'cy': {'query_text': 'Taith Bryn',\n",
       "  'reference_text': 'Euryn .',\n",
       "  'answer': 'Terfynol'},\n",
       " 'cdo': {'query_text': 'Lièng-hăk-guók : Dṳ̆ng-ùng',\n",
       "  'reference_text': 'Ngṳ̄-ngiòng Nĭk',\n",
       "  'answer': 'dâng-ciŭ'},\n",
       " 'zh-classical': {'query_text': '一 ︰ 紐',\n",
       "  'reference_text': '約 市 ；',\n",
       "  'answer': '約克時報'},\n",
       " 'sk': {'query_text': 'Do seniorského mužstva FC Nitra sa',\n",
       "  'reference_text': 'dostal ako 18 ročný .',\n",
       "  'answer': 'prebojoval v roku 2011.'}}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "results = {}\n",
    "\n",
    "# Disable datasets load_dataset progress bar\n",
    "disable_progress_bar()\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    seed = 0\n",
    "    random_language = random.choice(wikiann_languages)\n",
    "\n",
    "    wikiann = load_dataset(\"wikiann\", data_dir=random_language,\n",
    "                           download_mode=\"force_redownload\",\n",
    "                           verification_mode=\"no_checks\")\n",
    "\n",
    "    wikiann_samples = sample_from_dataset(wikiann, num_samples=1, seed=seed)['train']\n",
    "    \n",
    "    while len(wikiann_samples['tokens'][0]) <= 2:\n",
    "        seed += 1\n",
    "        wikiann_samples = sample_from_dataset(wikiann, num_samples=1, seed=seed)['train']\n",
    "        \n",
    "    tokens = wikiann_samples['tokens'][0]\n",
    "\n",
    "    results[random_language] = {}\n",
    "\n",
    "    # Cut-off - at least 2 tokens at the beginning\n",
    "    cut_off = int(np.ceil(len(tokens)/2))\n",
    "\n",
    "    query_text = ' '.join(tokens[:cut_off])\n",
    "    reference_text = ' '.join(tokens[cut_off:])\n",
    "    print(query_text, reference_text)\n",
    "\n",
    "    user_prompt = GUI_SENT.format(\n",
    "        split_name='train',\n",
    "        dataset_name='WikiAnn',\n",
    "        sentence=query_text\n",
    "    )\n",
    "\n",
    "    answer = ask_gpt(user_prompt, **ASK_GPT_PARAMS)\n",
    "\n",
    "    results[random_language]['query_text'] = query_text\n",
    "    results[random_language]['reference_text'] = reference_text\n",
    "    results[random_language]['answer'] = answer\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d1cc3e726ecd69fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:12:45.913776Z",
     "start_time": "2024-03-12T13:12:45.909669Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'WikiAnn_sentence_continuation.json'), 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "92815494729e9b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:12:45.931069Z",
     "start_time": "2024-03-12T13:12:45.915180Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = json.load(open(os.path.join(RESULTS_PATH, f'WikiAnn_sentence_continuation.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "494f4b667d6c5328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:12:51.972975Z",
     "start_time": "2024-03-12T13:12:45.932836Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b2d4361c0740c68fbc32950e6f8bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'No': 10})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_arr = []\n",
    "\n",
    "for id_, results_dict in tqdm(results.items(), total=10):\n",
    "\n",
    "    user_prompt = ICL_EVAL.format(\n",
    "        reference_text=results_dict['reference_text'],\n",
    "        candidate_text=results_dict['answer']\n",
    "    )\n",
    "    answer = ask_gpt(user_prompt, openai_client, temperature=TEMPERATURE, model='gpt-4')\n",
    "    results[id_]['match'] = answer\n",
    "\n",
    "    match_arr.append(answer)\n",
    "\n",
    "Counter(match_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883bd77252a2b20e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### WikiANN (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9ec4f63d05cc4e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:12:59.504829Z",
     "start_time": "2024-03-12T13:12:51.974690Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/ace (download: 2.85 MiB, generated: 8.92 MiB, post-processed: Unknown size, total: 11.78 MiB) to /root/.cache/huggingface/datasets/parquet/ace-ea28d0999d5d2e61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/ace-ea28d0999d5d2e61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiann = load_dataset(\"wikiann\", data_dir='en',\n",
    "                       download_mode=\"force_redownload\",\n",
    "                       verification_mode=\"no_checks\")\n",
    "wikiann_samples = sample_from_dataset(wikiann, num_samples=10)['train']\n",
    "wikiann_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4236fbe3ad5c6578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:10.591933Z",
     "start_time": "2024-03-12T13:12:59.506154Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7190944e69434b8d843578938aea7725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "results = {}\n",
    "\n",
    "for id_, tokens in tqdm(enumerate(wikiann_samples['tokens']), total=10):\n",
    "    results[id_] = {}\n",
    "\n",
    "    # Cut-off - at least 2 tokens at the beginning\n",
    "    cut_off = int(np.ceil(len(tokens)/2))\n",
    "\n",
    "    query_text = ' '.join(tokens[:cut_off])\n",
    "    reference_text = ' '.join(tokens[cut_off:])\n",
    "\n",
    "    user_prompt = GUI_SENT.format(\n",
    "        split_name='train',\n",
    "        dataset_name='WikiAnn',\n",
    "        sentence=query_text\n",
    "    )\n",
    "\n",
    "    answer = ask_gpt(user_prompt, **ASK_GPT_PARAMS)\n",
    "\n",
    "    results[id_]['query_text'] = query_text\n",
    "    results[id_]['reference_text'] = reference_text\n",
    "    results[id_]['answer'] = answer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a6e13d41fddf6790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:10.599785Z",
     "start_time": "2024-03-12T13:13:10.594268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'WikiAnn_eng_sentence_continuation.json'), 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5a4534bbb0b7949a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:10.605700Z",
     "start_time": "2024-03-12T13:13:10.601366Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = json.load(open(os.path.join(RESULTS_PATH, f'WikiAnn_eng_sentence_continuation.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8cb134510b41e774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:10.621400Z",
     "start_time": "2024-03-12T13:13:10.606941Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e6b3c99fb54ceda09818446944ee58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Start of the sentence:\n",
      " Stratford ,\n",
      "--> Reference (ground truth):\n",
      "Oklahoma\n",
      "--> Model's prediction:\n",
      "a district in the East End of London ,\n",
      "\n",
      "--> Start of the sentence:\n",
      " Anders Behring\n",
      "--> Reference (ground truth):\n",
      "Breivik\n",
      "--> Model's prediction:\n",
      "Breivik\n",
      "\n",
      "--> Start of the sentence:\n",
      " '' The World as Best as I Remember\n",
      "--> Reference (ground truth):\n",
      "It , Volume Two '' ( 1992 )\n",
      "--> Model's prediction:\n",
      "It ''\n",
      "\n",
      "--> Start of the sentence:\n",
      " 24 Apr –\n",
      "--> Reference (ground truth):\n",
      "Heinrich Himmler\n",
      "--> Model's prediction:\n",
      "2008 –\n",
      "\n",
      "--> Start of the sentence:\n",
      " `` Flight of Icarus ''\n",
      "--> Reference (ground truth):\n",
      "( Tierra Santa )\n",
      "--> Model's prediction:\n",
      "is the third track on the 1983 studio album Piece of Mind by the British heavy metal band Iron Maiden.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Bath and North\n",
      "--> Reference (ground truth):\n",
      "East Somerset\n",
      "--> Model's prediction:\n",
      "East Somerset\n",
      "\n",
      "--> Start of the sentence:\n",
      " Communes of the\n",
      "--> Reference (ground truth):\n",
      "Marne department\n",
      "--> Model's prediction:\n",
      "Canton of Geneva\n",
      "\n",
      "--> Start of the sentence:\n",
      " 1970 — did\n",
      "--> Reference (ground truth):\n",
      "not compete\n",
      "--> Model's prediction:\n",
      "not enter\n",
      "\n",
      "--> Start of the sentence:\n",
      " In Djibouti city , for instance , average\n",
      "--> Reference (ground truth):\n",
      "afternoon highs range from in April .\n",
      "--> Model's prediction:\n",
      "temperatures range between 20 and 35 degrees Celsius.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Prince Valdemar\n",
      "--> Reference (ground truth):\n",
      "of Denmark\n",
      "--> Model's prediction:\n",
      "of Denmark\n"
     ]
    }
   ],
   "source": [
    "for id_, results_dict in tqdm(results.items()):\n",
    "    print('--> Start of the sentence:\\n', results_dict['query_text'])\n",
    "    print(\"--> Reference (ground truth):\")\n",
    "    print(results_dict['reference_text'])\n",
    "    print(\"--> Model's prediction:\")\n",
    "    print(results_dict['answer'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a5aed97f4f4a1cfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:17.176526Z",
     "start_time": "2024-03-12T13:13:10.622330Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd2c102b23c4d6ea5be1b6170d9168e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'No': 6, 'Yes (exact match)': 3, 'Yes (near-exact match)': 1})"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_arr = []\n",
    "\n",
    "for id_, results_dict in tqdm(results.items(), total=10):\n",
    "\n",
    "    user_prompt = ICL_EVAL.format(\n",
    "        reference_text=results_dict['reference_text'],\n",
    "        candidate_text=results_dict['answer']\n",
    "    )\n",
    "    answer = ask_gpt(user_prompt, openai_client, temperature=TEMPERATURE, model='gpt-4')\n",
    "    results[id_]['match'] = answer\n",
    "\n",
    "    match_arr.append(answer)\n",
    "\n",
    "Counter(match_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3544edf7d4bdf89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### masakhaner2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "32289432d769bb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:17.180669Z",
     "start_time": "2024-03-12T13:13:17.177855Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Languages that were added to the second version of the masakhaner2\n",
    "masakhaner2_languages = ['bam', 'ewe', 'fon', 'twi', 'bbj', 'nya', 'tsn', 'sna', 'xho', 'zul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d6f41cc2201135a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:13:17.184563Z",
     "start_time": "2024-03-12T13:13:17.181667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GUI_SENT = \"\"\"INSTRUCTION:\n",
    "You are provided with the FIRST PIECE of data sample tokens in {language} from the {split_name} split of the {dataset_name} dataset. Finish the SECOND PIECE of the sample tokens as EXACTLY appeared in the dataset. ONLY rely on the original form of the sample tokens in the dataset to finish the SECOND PIECE. Do not return labels, provide only tokens exactly as they appear in the dataset.\n",
    "\n",
    "FIRST PIECE:\n",
    "{sentence}\n",
    "\n",
    "SECOND PIECE:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e229ad6cd1651607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:14:48.219944Z",
     "start_time": "2024-03-12T13:13:17.185773Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13906d682cea4308832819bf6902a4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yaʋaladzinɔnɔ naa be mieŋlɔa mia amedzesiwo be .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/bam/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEYE perezidan an'a Sekeretɛri zenerali ka ladilikanw fɛ , laɲini yɛlɛmana k'a kɛ Bolofara Lajɛ balalen dɔ de kɔni ɲinini ye , min bɛna laban ne binni na .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/bbj/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philémon Yang biŋ cə́ŋtə́ ntútə́ púa mtiŋ myə ǎ bɔ dá'tə gaə́ dyə̂fa' Ntwɔ́kshwɛ Atou dzʉ́ pè nə̂ mu' pə́púŋ tə a bó dɔ́ .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/twi/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ɔmanpanin Akufo - Addo se , ɔbɛyɛ deɛ ɔbɛtumi biara de asi galamsey ano ansa na ne berɛ a ɛtɔ so mmienu no atwam ɔpɛpɔn , 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/twi/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amnyɔkuo no panyin , Nelson ɔhamisa , aka sɛ ɔbɛkɔ so ako ama nsesa aba ɔman no mu ɛmfa ho ne nnipa binom a wɔkye wɔn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/fon/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kplékplé gbɛ̀tà gbɛ ɔ́ tɔn , sɔ́ azǎn mɔ̀ nyínkɔ́ tɔn ɖó é kplé xá Jeux Olimpique Modernes è blǒ ɖo azǎn 06ɔ́ lidòsùn xwè 1896ɔ́ tɔn è blǒ ɖo Athènes èe wú .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esi wonye be yeƒe hatruiawoe nana wolena , de , = = nu ŋuti la esia kpe ɖe eŋu wogale amewo lem .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/xho/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kwaba bantu babhubhileyo , amashumi amathandathu anesithandathu ( 66 ) kubo ngabantwana .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/ewe/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Journée FIFA : Guinée ƒe afɔbɔlƒolawo wɔ dɔgbegbenɔnɔme kuɖe woƒe tamega aɖewo ƒe matumatu ŋuti .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset masakhaner2 (/root/.cache/huggingface/datasets/masakhane___masakhaner2/zul/1.0.0/f5700a8b1468e7e03e17fe87897dca01d2eb8db3f67de132ada8396091a42963)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okukhulu kunakho konke uMotsepe akufunayo , ukuqala kokusetshenziswa kwemishini ye - Video Assistant Referee ( VAR ) .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'query_text': 'Yaʋaladzinɔnɔ naa be mieŋlɔa',\n",
       "  'reference_text': 'mia amedzesiwo be .',\n",
       "  'answer': 'mɔ ko dziƒome.'},\n",
       " 1: {'query_text': \"SEYE perezidan an'a Sekeretɛri zenerali ka ladilikanw fɛ , laɲini yɛlɛmana k'a kɛ Bolofara Lajɛ\",\n",
       "  'reference_text': 'balalen dɔ de kɔni ɲinini ye , min bɛna laban ne binni na .',\n",
       "  'answer': \"Kɔnɔ Kɛnɛdugu la, 1995 bɛ bi kalo fe, n'tɛmɛnenw bɛɛ sɔrɔ sɔnifɛnna to.\"},\n",
       " 2: {'query_text': \"Philémon Yang biŋ cə́ŋtə́ ntútə́ púa mtiŋ myə ǎ bɔ dá'tə gaə́ dyə̂fa'\",\n",
       "  'reference_text': \"Ntwɔ́kshwɛ Atou dzʉ́ pè nə̂ mu' pə́púŋ tə a bó dɔ́ .\",\n",
       "  'answer': \"mbiə́ məkɔ́'tə mə̀ nzɨ́nzɨ́ mə̀ ghɨə́nə mə̀ bǎŋkə mə̀ bɨǎ mfiə́ mə̀ bɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨa\"},\n",
       " 3: {'query_text': 'ɔmanpanin Akufo - Addo se , ɔbɛyɛ deɛ ɔbɛtumi biara de asi galamsey ano',\n",
       "  'reference_text': 'ansa na ne berɛ a ɛtɔ so mmienu no atwam ɔpɛpɔn , 2015',\n",
       "  'answer': 'mu ho adi no, na ɛsɛ sɛ ɔmanfoɔ nyinaa betumi ahyɛ den na wɔatumi agye Ghana nkɔso.'},\n",
       " 4: {'query_text': 'Amnyɔkuo no panyin , Nelson ɔhamisa , aka sɛ ɔbɛkɔ so ako ama',\n",
       "  'reference_text': 'nsesa aba ɔman no mu ɛmfa ho ne nnipa binom a wɔkye wɔn',\n",
       "  'answer': 'Ghana mpoano no mu akyirikyiri biako.'},\n",
       " 5: {'query_text': 'Kplékplé gbɛ̀tà gbɛ ɔ́ tɔn , sɔ́ azǎn mɔ̀ nyínkɔ́ tɔn ɖó é kplé xá Jeux Olimpique',\n",
       "  'reference_text': 'Modernes è blǒ ɖo azǎn 06ɔ́ lidòsùn xwè 1896ɔ́ tɔn è blǒ ɖo Athènes èe wú .',\n",
       "  'answer': 'de Rio 2016, ɖò àtɔnmɛ wú.'},\n",
       " 6: {'query_text': 'Esi wonye be yeƒe hatruiawoe nana wolena , de , = =',\n",
       "  'reference_text': 'nu ŋuti la esia kpe ɖe eŋu wogale amewo lem .',\n",
       "  'answer': 'aɖeke me , na nye ƒe aɖe bubuwo me .'},\n",
       " 7: {'query_text': 'Kwaba bantu babhubhileyo , amashumi amathandathu anesithandathu',\n",
       "  'reference_text': '( 66 ) kubo ngabantwana .',\n",
       "  'answer': 'abantu abadala .'},\n",
       " 8: {'query_text': 'Journée FIFA : Guinée ƒe afɔbɔlƒolawo wɔ dɔgbegbenɔnɔme',\n",
       "  'reference_text': 'kuɖe woƒe tamega aɖewo ƒe matumatu ŋuti .',\n",
       "  'answer': 'na Ghana nye amesiwo ƒe afɔbɔlƒolawo me.'},\n",
       " 9: {'query_text': 'Okukhulu kunakho konke uMotsepe akufunayo , ukuqala kokusetshenziswa kwemishini',\n",
       "  'reference_text': 'ye - Video Assistant Referee ( VAR ) .',\n",
       "  'answer': 'yokumbiwa phansi eMzansi Africa.'}}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "results = {}\n",
    "\n",
    "\n",
    "# Disable datasets load_dataset progress bar\n",
    "disable_progress_bar()\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    seed = 0\n",
    "    random_language = random.choice(masakhaner2_languages)\n",
    "\n",
    "    masaknaner2 = load_dataset(config['dataset'], random_language)\n",
    "\n",
    "    masaknaner2_samples = sample_from_dataset(masaknaner2, num_samples=1, seed=i)['train']\n",
    "\n",
    "    while len(masaknaner2_samples['tokens'][0]) <= 2:\n",
    "        seed *= 100\n",
    "        masaknaner2_samples = sample_from_dataset(masaknaner2, num_samples=1, seed=seed)['train']\n",
    "\n",
    "    tokens = masaknaner2_samples['tokens'][0]\n",
    "\n",
    "    results[i] = {}\n",
    "\n",
    "    # Cut-off - at least 2 tokens at the beginning\n",
    "    cut_off = int(np.ceil(len(tokens)/2))\n",
    "\n",
    "    query_text = ' '.join(tokens[:cut_off])\n",
    "    reference_text = ' '.join(tokens[cut_off:])\n",
    "    print(query_text, reference_text)\n",
    "\n",
    "    user_prompt = GUI_SENT.format(\n",
    "        split_name='train',\n",
    "        dataset_name='MasakhaNER 2.0',\n",
    "        sentence=query_text,\n",
    "        language=config['languages_names'][random_language]\n",
    "    )\n",
    "\n",
    "    answer = ask_gpt(user_prompt, **ASK_GPT_PARAMS)\n",
    "\n",
    "    results[i]['query_text'] = query_text\n",
    "    results[i]['reference_text'] = reference_text\n",
    "    results[i]['answer'] = answer\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "97a3b6251b878f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:14:48.225422Z",
     "start_time": "2024-03-12T13:14:48.221584Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, f'masakhaner2_sentence_continuation.json'), 'w') as file:\n",
    "    json.dump(results, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "1356cf599f2a0635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:14:48.230529Z",
     "start_time": "2024-03-12T13:14:48.226863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = json.load(open(os.path.join(RESULTS_PATH, f'masakhaner2_sentence_continuation.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5a6d78a6c2dccb47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:14:48.248986Z",
     "start_time": "2024-03-12T13:14:48.231912Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d78a74cae0463ebdd502c6f903fdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Start of the sentence:\n",
      " Yaʋaladzinɔnɔ naa be mieŋlɔa\n",
      "--> Reference (ground truth):\n",
      "mia amedzesiwo be .\n",
      "--> Model's prediction:\n",
      "mɔ ko dziƒome.\n",
      "\n",
      "--> Start of the sentence:\n",
      " SEYE perezidan an'a Sekeretɛri zenerali ka ladilikanw fɛ , laɲini yɛlɛmana k'a kɛ Bolofara Lajɛ\n",
      "--> Reference (ground truth):\n",
      "balalen dɔ de kɔni ɲinini ye , min bɛna laban ne binni na .\n",
      "--> Model's prediction:\n",
      "Kɔnɔ Kɛnɛdugu la, 1995 bɛ bi kalo fe, n'tɛmɛnenw bɛɛ sɔrɔ sɔnifɛnna to.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Philémon Yang biŋ cə́ŋtə́ ntútə́ púa mtiŋ myə ǎ bɔ dá'tə gaə́ dyə̂fa'\n",
      "--> Reference (ground truth):\n",
      "Ntwɔ́kshwɛ Atou dzʉ́ pè nə̂ mu' pə́púŋ tə a bó dɔ́ .\n",
      "--> Model's prediction:\n",
      "mbiə́ məkɔ́'tə mə̀ nzɨ́nzɨ́ mə̀ ghɨə́nə mə̀ bǎŋkə mə̀ bɨǎ mfiə́ mə̀ bɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨǎ mfiə́ mə̀ dzɨa\n",
      "\n",
      "--> Start of the sentence:\n",
      " ɔmanpanin Akufo - Addo se , ɔbɛyɛ deɛ ɔbɛtumi biara de asi galamsey ano\n",
      "--> Reference (ground truth):\n",
      "ansa na ne berɛ a ɛtɔ so mmienu no atwam ɔpɛpɔn , 2015\n",
      "--> Model's prediction:\n",
      "mu ho adi no, na ɛsɛ sɛ ɔmanfoɔ nyinaa betumi ahyɛ den na wɔatumi agye Ghana nkɔso.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Amnyɔkuo no panyin , Nelson ɔhamisa , aka sɛ ɔbɛkɔ so ako ama\n",
      "--> Reference (ground truth):\n",
      "nsesa aba ɔman no mu ɛmfa ho ne nnipa binom a wɔkye wɔn\n",
      "--> Model's prediction:\n",
      "Ghana mpoano no mu akyirikyiri biako.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Kplékplé gbɛ̀tà gbɛ ɔ́ tɔn , sɔ́ azǎn mɔ̀ nyínkɔ́ tɔn ɖó é kplé xá Jeux Olimpique\n",
      "--> Reference (ground truth):\n",
      "Modernes è blǒ ɖo azǎn 06ɔ́ lidòsùn xwè 1896ɔ́ tɔn è blǒ ɖo Athènes èe wú .\n",
      "--> Model's prediction:\n",
      "de Rio 2016, ɖò àtɔnmɛ wú.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Esi wonye be yeƒe hatruiawoe nana wolena , de , = =\n",
      "--> Reference (ground truth):\n",
      "nu ŋuti la esia kpe ɖe eŋu wogale amewo lem .\n",
      "--> Model's prediction:\n",
      "aɖeke me , na nye ƒe aɖe bubuwo me .\n",
      "\n",
      "--> Start of the sentence:\n",
      " Kwaba bantu babhubhileyo , amashumi amathandathu anesithandathu\n",
      "--> Reference (ground truth):\n",
      "( 66 ) kubo ngabantwana .\n",
      "--> Model's prediction:\n",
      "abantu abadala .\n",
      "\n",
      "--> Start of the sentence:\n",
      " Journée FIFA : Guinée ƒe afɔbɔlƒolawo wɔ dɔgbegbenɔnɔme\n",
      "--> Reference (ground truth):\n",
      "kuɖe woƒe tamega aɖewo ƒe matumatu ŋuti .\n",
      "--> Model's prediction:\n",
      "na Ghana nye amesiwo ƒe afɔbɔlƒolawo me.\n",
      "\n",
      "--> Start of the sentence:\n",
      " Okukhulu kunakho konke uMotsepe akufunayo , ukuqala kokusetshenziswa kwemishini\n",
      "--> Reference (ground truth):\n",
      "ye - Video Assistant Referee ( VAR ) .\n",
      "--> Model's prediction:\n",
      "yokumbiwa phansi eMzansi Africa.\n"
     ]
    }
   ],
   "source": [
    "for id_, results_dict in tqdm(results.items()):\n",
    "    print('--> Start of the sentence:\\n', results_dict['query_text'])\n",
    "    print(\"--> Reference (ground truth):\")\n",
    "    print(results_dict['reference_text'])\n",
    "    print(\"--> Model's prediction:\")\n",
    "    print(results_dict['answer'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "eebaeb9e913ca816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-12T13:14:54.416042Z",
     "start_time": "2024-03-12T13:14:48.250282Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cea5c3444e346bf903bcc786813aeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'No': 10})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_arr = []\n",
    "\n",
    "for id_, results_dict in tqdm(results.items(), total=10):\n",
    "\n",
    "    user_prompt = ICL_EVAL.format(\n",
    "        reference_text=results_dict['reference_text'],\n",
    "        candidate_text=results_dict['answer']\n",
    "    )\n",
    "    answer = ask_gpt(user_prompt, openai_client, temperature=TEMPERATURE, model='gpt-4')\n",
    "    results[id_]['match'] = answer\n",
    "\n",
    "    match_arr.append(answer)\n",
    "\n",
    "Counter(match_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
